{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "This tutorial shows how to quickly get started using *scarlet* to model an hyperspectral image cube. For a more in-depth introduction to *scarlet*, read the [Core Concepts](1-concepts.ipynb) or the [API Documentation](api/index.rst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages and setup\n",
    "import numpy as np\n",
    "import scarlet\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use a good colormap and don't interpolate the pixels\n",
    "matplotlib.rc('image', cmap='inferno', interpolation='none', origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Display Data\n",
    "\n",
    "We load an example data set (here an image cube with 5 bands) *and* a detection catalog.\n",
    "If such a catalog is not available, packages like [SEP](http://sep.readthedocs.io/) and [photutils](https://photutils.readthedocs.io/en/stable/) will happily generate one, but for this example we use part of the detection catalog generated by the [LSST DM stack](https://github.com/lsst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample images\n",
    "data = np.load(\"../data/hsc_cosmos_35.npz\")\n",
    "images = data[\"images\"]\n",
    "filters = data[\"filters\"]\n",
    "centers = [(src['y'], src['x']) for src in data[\"catalog\"]] # Note: y/x convention!\n",
    "weights = 1/data[\"variance\"]\n",
    "psf = scarlet.ImagePSF(data[\"psfs\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "    Coordinates in *scarlet* are given in the C/numpy notation (y,x) as opposed to the more conventional mathematical (x,y) ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Image Cube\n",
    "\n",
    "This is an example of how to display an RGB image from an image cube of multiband data. In this case the image uses a $sinh^{-1}$ function to normalize the flux in each filter consistently to create an RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scarlet.display import AsinhMapping\n",
    "\n",
    "stretch = 0.2\n",
    "Q = 10\n",
    "norm = AsinhMapping(minimum=0, stretch=stretch, Q=Q)\n",
    "\n",
    "img_rgb = scarlet.display.img_to_rgb(images, norm=norm)\n",
    "plt.imshow(img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Frame and Observation\n",
    "\n",
    "A `Frame` in *scarlet* is a description of the hyperspectral cube of the model or the observations. Think of it as the metadata, what aspects of the sky are described here. At the least, a `Frame` holds the `shape` of the cube, for which we use the convention `(C, Ny, Nx)` for the number of elements in 3 dimensions: `C` for the number of bands/channels and `Ny, Nx` for the number of pixels at every channel.\n",
    "\n",
    "An `Observation` combines a `Frame` with several data units, similar to header-data arrangement in FITS files. In addition to the actual science image cube, you can and often must provide weights for all elements in the data cube, an image cube of the PSF model (one image for all or one for each channel), an `astropy.WCS` structure to translate from pixel to sky coordinates, and labels for all channels. The reason for specifying them is to enable the code to internally map from the model frame, in which you seek to fit a model, to the observed data frame."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    It is critical to realize that there are two frames: one for model that represents the scene on the sky, and one for any observation of that scene. The frames may be identical, but usually they are not, in which case the observation is compared to a degraded rendering of the model. The degradation could be in terms of spatial resolution, PSF blurring, spectral coverage, or all of the above. If the model frame and the observation frame are the same, no attempt can be made to recover information lost by the degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we assume that bands and pixel locations are identical between the model and the observation. Because we have ground-based images with different PSFs in each band, we need to provide a reference PSF for the model. We simply choose a minimal Gaussian PSF that is barely well sampled as our reference kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_psf = scarlet.GaussianPSF(sigma=(0.8,)*len(filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can fully specify the `Frame` and `Observation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frame = scarlet.Frame(\n",
    "    images.shape,\n",
    "    psf=model_psf,\n",
    "    channels=filters)\n",
    "\n",
    "observation = scarlet.Observation(\n",
    "    images, \n",
    "    psf=psf,\n",
    "    weights=weights, \n",
    "    channels=filters).match(model_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last command calls the `match` method to compute e.g. PSF difference kernel and filter transformations.\n",
    "\n",
    "We generally recommend this pattern:\n",
    "1. define model frame\n",
    "2. construct observation\n",
    "3. match it to the model frame\n",
    "\n",
    "Steps 2 and 3 are combined above using a fluent pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make use of the convenience function `scarlet.display.show_observation` to plot observations as RGB images, with individual sources labeled at their position on the sky. It can also show the PSF in the same scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarlet.display.show_observation(observation, norm=norm, sky_coords=centers, show_psf=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use a trivial `wcs` in this `Observation`, all coordinates are already in image pixels, otherwise RA/Dec pairs are expected as sky coordinates. Also:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize sources\n",
    "\n",
    "You now need to define sources that are going to be fit. The full model, which we will call `Blend`, is a collection of those sources. We provide several pre-built source types, e.g.:\n",
    "\n",
    "* `RandomSource` fits per-band amplitude and non-parametric morphology starting from uniform random draws for both.\n",
    "* `PointSource` fits centers and per-band amplitude using the observed PSF model.\n",
    "* `ExtendedSource` fits per-band amplitude and a non-parametric morphology (which is constrained to be monotonically decreasing from the center). It has a `compact` option, which initializes the source with the shape of a point source.\n",
    "* `ExtendedSource(K=2)` splits an `ExtendedSource` into `K` components that are initially radially separated and stacked on top of each other like a pyramid.\n",
    "\n",
    "We can use a robust initialization scheme, that starts by attempting to model every source with 2 components. If these components don't have enough signal-to-noise, it will fall back to 1 component. If that fails, it will work with a `compact` component. If cannot do even that, it will report the source number in `skipped`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, skipped = scarlet.initialization.init_all_sources(model_frame, \n",
    "                                                           centers, \n",
    "                                                           observation, \n",
    "                                                           max_components=2, \n",
    "                                                           min_snr=50,\n",
    "                                                           thresh=1,\n",
    "                                                           fallback=True,\n",
    "                                                           silent=True,\n",
    "                                                           set_spectra=True\n",
    "                                                          )\n",
    "\n",
    "for k, src in enumerate(sources):\n",
    "    print (f\"{k}: {src.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this method chooses multiple (here 2) extended components for the first three sources, a single extended component for sources 3 and 5, and compact ones for the rest.\n",
    "\n",
    "If we know something about the scene, we might want to customize the modeling. Let's assume that we know that object 0 is a star. We could just replace that source with a `PointSource`, but for clarity we rebuild the entire source list, making one change for source 0 and accepting everything else from the default initialization above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "for k,center in enumerate(centers):\n",
    "    if k == 0:\n",
    "        new_source = scarlet.PointSource(model_frame, center, observation)\n",
    "    elif k in [1, 2]:\n",
    "        new_source = scarlet.ExtendedSource(model_frame, center, observation, K=2)\n",
    "    elif k in [3, 5]:\n",
    "         new_source = scarlet.ExtendedSource(model_frame, center, observation, K=1)\n",
    "    else:\n",
    "         new_source = scarlet.ExtendedSource(model_frame, center, observation, compact=True)\n",
    "    sources.append(new_source)\n",
    "    \n",
    "for k, src in enumerate(sources):\n",
    "    print (f\"{k}: {src.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sources are initialized independently, with spectra taken from their peak position in the observations. We can make another sweep and determine their spectra such that their superposition best matches the observation. Above, this was done for use by the option `set_specta=True`. But we can call the linear solver directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarlet.initialization.set_spectra_to_match(sources, observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit Model\n",
    "\n",
    "The `Blend` class holds the list of sources and has the machinery to fit them to the given images. In this example the code is set to run for a maximum of 100 iterations, but will end early if the likelihood and all of the constraints converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend = scarlet.Blend(sources, observation)\n",
    "%time it, logL = blend.fit(100, e_rel=1e-4)\n",
    "print(f\"scarlet ran for {it} iterations to logL = {logL}\")\n",
    "scarlet.display.show_likelihood(blend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Full Scene\n",
    "\n",
    "We could use `scarlet.display.show_scene` to render to entire scene, but it's instructive to see how the model and the comparison to observations is performed.\n",
    "First we load the model for the entire scene, render it in the observation frame, and compute its residuals. We then show model and data with the same $sinh^{-1}$ stretch and the residuals with a linear stretch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model\n",
    "model = blend.get_model()\n",
    "# Render it in the observed frame\n",
    "model_ = observation.render(model)\n",
    "# Compute residual\n",
    "residual = images-model_\n",
    "\n",
    "# Create RGB images\n",
    "model_rgb = scarlet.display.img_to_rgb(model_, norm=norm)\n",
    "residual_rgb = scarlet.display.img_to_rgb(residual)\n",
    "\n",
    "# Show the data, model, and residual\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = [fig.add_subplot(1,3,n+1) for n in range(3)]\n",
    "ax[0].imshow(img_rgb)\n",
    "ax[0].set_title(\"Data\")\n",
    "ax[1].imshow(model_rgb)\n",
    "ax[1].set_title(\"Model\")\n",
    "ax[2].imshow(residual_rgb)\n",
    "ax[2].set_title(\"Residual\")\n",
    "\n",
    "for k,src in enumerate(blend):\n",
    "    if hasattr(src, \"center\"):\n",
    "        y,x = src.center\n",
    "        ax[0].text(x, y, k, color=\"w\")\n",
    "        ax[1].text(x, y, k, color=\"w\")\n",
    "        ax[2].text(x, y, k, color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Source Models\n",
    "\n",
    "\n",
    "We will now inspect the model for each source, in its original frame and in its observed frame by leveraging the `show_sources` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarlet.display.show_sources(sources, \n",
    "                             norm=norm, \n",
    "                             observation=observation,\n",
    "                             show_model=True,\n",
    "                             show_rendered=True, \n",
    "                             show_observed=True,\n",
    "                             add_markers=False,\n",
    "                             add_boxes=True\n",
    "                            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each source \"lives\" in a smaller box and is then placed into the larger scene. The model of object 0 assumes the simple Gaussian shape of the model PSF, which is the internal representation of a point source. Source 1 uses the freedom of the 2-compoent model to represent a slightly redder core; the difference in spectrum is clearly noticeable.\n",
    "\n",
    "### Measure Fluxes\n",
    "\n",
    "The color information in these plots stems from the per-band amplitude, which are computed from the hyperspectral model by integrating over the morphology. The source spectra plots in the right panels above have done exactly that. The convention of these fluxes is given by the units and ordering of the original data cube. In the case of multi-component sources, the fluxes of all components are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"----------------- {}\".format(filters))\n",
    "for k, src in enumerate(sources):\n",
    "    print (\"Source {}, Fluxes: {}\".format(k, scarlet.measure.flux(src)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other measurements (e.g. `centroid`) are also implemented. The measurement approach is also easily extendable. The source models are generated in the model frame (which is the best-fit representation of the full hyperspectral `Frame`), from which any measurement can directly be made without having to deal with noise, PSF convolution, overlapping sources, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Re-Use Model\n",
    "\n",
    "To preserve the model for posterity, individual sources or lists of sources can be serialized with the `pickle` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open(\"hsc_cosmos_35.sca\", \"wb\")\n",
    "pickle.dump(sources, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    We recommend to serialize sources or source lists instead of a blend because the latter would also serialize the observations. That is normally not desired and will bloat the pickled file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickled file can be reopened in the same way. Every source can be utilized as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"hsc_cosmos_35.sca\", \"rb\")\n",
    "sources_ = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "scarlet.display.show_scene(sources_, norm=norm, add_boxes=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want to refit some data with this model, we have to recreate a `Blend` instance.\n",
    "\n",
    "We will now add two more sources to account for the largest residuals we have seen above. As we don't know their location accurately, we allow the fitter to shift/recenter the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two sources at their approximate locations\n",
    "model_frame_ = sources_[0].frame\n",
    "yx = (14., 44.)\n",
    "new_source = scarlet.ExtendedSource(model_frame_, yx, observation, compact=True, shifting=True)\n",
    "sources_.append(new_source)\n",
    "yx = (42., 9.)\n",
    "new_source = scarlet.ExtendedSource(model_frame_, yx, observation, compact=True, shifting=True)\n",
    "sources_.append(new_source)\n",
    "\n",
    "# generate a new Blend instance\n",
    "blend_ = scarlet.Blend(sources_, observation)\n",
    "# tighten relative change in likelihood for convergence\n",
    "%time it_, logL_ = blend_.fit(100, e_rel=1e-4)\n",
    "print(f\"scarlet ran for {it_} iterations to logL = {logL_}\")\n",
    "scarlet.display.show_likelihood(blend_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that logL is slightly higher than before. Let's have a look at the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scarlet.display.show_scene(sources_, \n",
    "                           norm=norm, \n",
    "                           observation=observation, \n",
    "                           show_rendered=True, \n",
    "                           show_observed=True, \n",
    "                           show_residual=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the two new sources 7 and 8, and that most of the features are very similar to before. As expected, the residuals have visibly improved and are now dominated by a red-blue pattern in the center of source 1, which we already fit with 2 components. Maybe we should try with 3 ..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
