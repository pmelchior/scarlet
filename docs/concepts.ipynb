{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Concepts\n",
    "\n",
    "The purpose of this guide is to explain the core concepts of *scarlet*, how they are used and how they can be extended and customized for more specialized science cases.\n",
    "\n",
    "Other resources are:\n",
    "\n",
    "1. Our [Quickstart Guide](quickstart) shows a typical *scarlet* session.\n",
    "2. The [API Documentation](api/scarlet.rst) describes modules and classes of the python library.\n",
    "3. A more in-depth explaination of the mathematics and algorithms used by *scarlet* is in [Melchior et al. 2018](https://arxiv.org/abs/1802.10157)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The goal of *scarlet* is to create a model of individual astrophysical sources from a collection of observations of a rectangular region of the sky. These observations can be in multiple filter bands (which we will call \"channels\" internally) with different PSFs, from telescopes with different resolutions, and eventually even spectroscopic instruments.\n",
    "A main emphasis of *scarlet* lies in deblending overlapping sources. \n",
    "As is pointed out by Robert Lupton, perfect reconstruction of a blended scene is [impossible](https://docushare.lsst.org/docushare/dsweb/Services/Document-29071), however by making a few minor assumptions *scarlet* improves on other blending algorithms by leveraging as much data as possible.\n",
    "\n",
    "The basic assumption of *scarlet* is that the sources in an astrophysical image can be thought of as a collection of `Component` objects, where each component has its own internal representation of the source properties (non-parameteric or parametric), from which it computes its model of the sources in a 3D hyper-spectral data cube of pixels vs wavelegth.\n",
    "A common model is `FactorizedComponent`, which describes the source as a non-parametric morphology image and an intensity in every channel (SED).\n",
    "With this ansatz, more complicated objects, like galaxies, can be thought of as a combination multiple components (a `ComponentTree`), where components with different SEDs represent different populations of stars or gases in the host galaxy. To properly separate sources, further assumptions are required, for example that all of the flux is positive and that the spatial distribution stars and galaxies monotonically decrease from their centers.\n",
    "\n",
    "A `Frame` contains metadata for the hyperspectral cube *scarlet* seeks to construct as well as those describing an `Observation`. The latter is the combination of a `Frame` with several data units. Each observation can have multiple filter bands with a different PSF in each band that is internally matched to the model PSF.\n",
    "\n",
    "Into the model frame one or multiple `Component`s are inserted. Each of them can create a model of the hyperspectral data cube from its `Parameter`s. The recommended way of interacting with components is through the `source` classes.\n",
    "\n",
    "Finally, the `Blend` class links the sources with the observations and executes the optimization algorithm. \n",
    "\n",
    "For the most common type of source, we assume that the hyperspectral cube can be factorized into a 1D SED and a 2D morphology. Mathematically, the model of the scene is then\n",
    "\n",
    "$$\\mathsf{M}= \\sum_{k=1}^K \\mathsf{A}_k^T \\times \\mathsf{S}_k = \\mathsf{A}\\mathsf{S}, $$\n",
    "\n",
    "where $\\mathsf{A}_k \\in \\mathbb{R}^C$ is the normalized SED and $\\mathsf{S}_k \\in \\mathbb{R}^N$ is the morphology of a single component in the model with $C$ channels and $N$ pixels in each channel.\n",
    "It is important to note that this so-called matrix factorization implies that SEDs and morphologies are independent, e.g. the SED of a component does not change over the region covered by its morphology.\n",
    "\n",
    "The scene is fit by minimizing the log-likelihood of the model, namely minimizing\n",
    "\n",
    "$$f(\\mathsf{A},\\mathsf{S}) \\propto \\frac{1}{2} || \\mathsf{Y}-\\mathsf{A}\\mathsf{S} ||_2^2, $$\n",
    "\n",
    "where $\\mathsf{Y}$ is a data cube and $||.||_2$ is the element-wise $L_2$ (Frobenius) norm. There is one such term for every observation.\n",
    "In detail, weights and other transformations like PSF convolutions also enter here, but as long as the noise is additive and Gaussian, the general form of a quadratic log-likelihood holds.\n",
    "\n",
    "Because there are often strong degeneracies between model components and their parameters, we exploit two mechanisms to stabilize the inference.\n",
    "* Every component parameter can specify a differentiable (log-)prior distribution.\n",
    "* Every component parameter can further be constraint non-differentiable penalties.\n",
    "\n",
    "Both options turn the inference of the maximum-likelihood estimate into a maximum a posteriori (MAP) estimate by minimizing\n",
    "$$f(\\mathsf{A}, \\mathsf{S}) + \\sum_{k=1}^K \\sum_{m=1}^{M_k} g^A_{km} \\left(\\mathsf{A}_{km} \\right) + g^S_{km} \\left(\\mathsf{S}_{km} \\right)$$\n",
    "\n",
    "\n",
    "While we optimize the log-likelihood and the log-prior by gradient descent, hard constraints are enforced through proximal operators; the curious reader will find more details in [Parikh & Boyd 2014](http://www.web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) and [Combettes & Pesquet 2011](https://link.springer.com/chapter/10.1007/978-1-4419-9569-8_10).\n",
    "In short, proximal operators map an input vector to the nearest vector that satisfied the respective constraint. Many constraints/penalty functions have analytic proximal operators.\n",
    "The entire optimization uses the adaptive proximal gradient method (a non-smooth generalization of the popular Adam method) from the [proxmin](https://github.com/pmelchior/proxmin) package, described in [Melchior et al. 2019](https://arxiv.org/abs/1910.10094)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame\n",
    "\n",
    "*tbd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how to construct a custom source"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "    Coordinates in *scarlet* are given in the C/numpy notation (y,x) as opposed to the more conventional mathematical (x,y) ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component\n",
    "\n",
    "*tbd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter\n",
    "\n",
    "Internally, *scarlet* solves an optimization problem, namely reducing the loss by adjusting the parameters of each component. The loss is the log-likelihood of the observed data given the model. Every component can declare its own parameters, which we can access by with the `parameters` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,src in enumerate(sources):\n",
    "    for p in src.parameters:\n",
    "        print (\"Source {}, Parameter shape {}, Converged {}\".format(k, p.shape, p.converged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter with length 5 is the SED, while the other describes the morphology. For object 0, this is simple a 2D center, the rest use images of different sizes.\n",
    "\n",
    "Each parameter is a souped up `numpy` array. It has attributes that store priors and constraints that were enforced during optimization; whether this parameter is considered converged; and an error estimate. In our example, several parameters have converged within relative changes of `e_rel=1e-3` (the default setting of `Blend.fit`), but others have not. This is why the fitter complained about non-convergence. The run above stopped because the loss did not change noticeably anymore.\n",
    "\n",
    "To demonstrate the use of error estimate, we make a signal-to-noise map of the morphology of source 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sources[5].parameters[1]\n",
    "plt.imshow(p / p.std)\n",
    "plt.colorbar(label='SNR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNR map shows that the center region is well determined by the data. However, this error estimate is purely statistical and does not include correlations between different parameters or different components. In fact, there's an upper lobe in the top-left corner of this source that is part of source 0. The gradient optimizer would exploit that and increase the morphology values there, but the monotonicity constraint has largely prevented that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior\n",
    "\n",
    "`Prior` and `Constraint` bring in additional information, which helps with robust parameter inference, which is especially important for cases with strong parameter degeneracies. Blending inevitably yields such degeneracies.\n",
    "\n",
    "`Prior`s encode which values for a parameter are more likely. The optimimization adds the log-likelihood and the log-prior to find a MAP estimate.\n",
    "\n",
    "`Prior`s need to implement two methods:\n",
    "* `__call__(self, x)` returns the logarithm of the prior at the value `x` of the parameter\n",
    "* `grad(self, x)` returns the gradient of the logarithm of the prior at `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint\n",
    "\n",
    "`Constraint`s appear similar to `Prior`s, in that they describe prior information about valid parameter values. However, they differ in profound ways. A wide class of constraints are projections onto submanifolds of the parameter space, typically guided by theoretical knowledge or assumptions. Examples are the subspace of positive elements or the surface of a sphere. This means that solutions outside of the manifold are forbidden, while all solution on the manifold are equally acceptable. The transition between these two cases is generally non-differentiable. This requires a generalization of gradient-based optimizers to so-called sub-gradients, and the employment of proximal operators.\n",
    "\n",
    "`Constraint`s need to implement only one method: `__call__(self, x, step)` which returns the result of the proximal mapping for a parameter with value `x`. For projection operators, that amounts to the point on the desired manifold that is closest to `x` in the Euclidean metric. The argument `step` is the step size for the current gradient step, which is only used for some classes for proximal operators. Conveniently, projection operators *don't* use it.\n",
    "\n",
    "*scarlet* implements several proximal constraints, some of which we discuss below. In addition, L1 and L0 sparsity penalties are implemented. More can be added by exploiting analytical results e.g. in [Parikh & Boyd 2014](http://www.web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) and [Combettes & Pesquet 2011](https://link.springer.com/chapter/10.1007/978-1-4419-9569-8_10). Open an [issue](https://github.com/pmelchior/scarlet/issues) for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positivity\n",
    "\n",
    "`PositivityConstraint` avoids the negative subspace. It performs the mapping $x\\rightarrow \\max(0, x)$ on every element of a parameter.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "\n",
    "For `FactorizedComponent`, a fundamental degeneracy arises from the question, which of the factors should capture the amplitude of the model. The overall flux could be stored in the SED or in the morphology, or some combination of both. The last option is degenerate.\n",
    "\n",
    "`NormalizationConstraint` allows the normalize the parameter, either to the sum of all elements or to the maximum element. In *scarlet*, we usually normalized the component morphology with `type='max'`, which can be very easily initialized without regarding the extent of the source. It results in the SED encoding color and intensity information. The advantage of this normalization is that two sources with similar colors can still be distinguished if they have different intensities.\n",
    "\n",
    "### Symmetry\n",
    "\n",
    "Demanding that astrophysical sources are symmetric reduces the number of effective degrees of freedom of the model by half, and most galaxies are in fact *largely* symmetric. The idea of using symmetry as a constraint has been used successfully in the SDSS deblender and also in our tests on substantially deeper HSC images. The proximal mapping for every symmetric pair of pixels $i,j$ is $x_i,x_j\\rightarrow\\tfrac{1}{2}(x_i + x_j)$.\n",
    "\n",
    "To make a source symmetric requires a position to make the model symmetric about. Source models are highy sensitive to this fractional pixel location so it is necessary to include an update function that estimates the position of a symmetric source in the blend. This operation is expensive, so the stability of this constrained needs to be weighed against the cost of enforcing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonicity\n",
    "\n",
    "Another useful constraint from the SDSS-HSC deblender is the assumption that most astrophysical objects are monotonically decreasing from the peak.\n",
    "In detail, this assumption is incorrect e.g. for spiral galaxies, especially tightly wound ones.\n",
    "But we can build good representations of even complex galaxies as multiple stellar populations, each with a single SED and monotonically decreasing from its peak.\n",
    "\n",
    "In *scarlet* monotonicity is implemented as a projection that is *not* a true proximal operator. It is possible to write monotonicity as a true proximal operator, but the implementation is far too slow for practical purposes. Instead, the morphology is projected into a space that has *a* monotonic solution, just not the closest one. We implemented two possible monotonic solutions in `MonotonicityConstraint`. If `use_nearest=True`, then only a single reference pixel is used: the nearest one in the direction to the peak.\n",
    "With `use_nearest=False` a weighted average of all pixels closer to the peak than the current pixel is used to allow for a smoother monotonic solution.\n",
    "\n",
    "| ![](images/nearest_ref.png) | ![](images/weighted_ref.png) |\n",
    "|:---------------------------:|:----------------------------:|\n",
    "| Nearest Neighbor            | Weighted Reference           |\n",
    "\n",
    "\n",
    "The monotonicity mapping hinges on a properly centered source, but in contrast to symmetry the center has to be localized only within one pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint Chains\n",
    "\n",
    "If multiple constraints should be simultaneously enforced, they can be organized in a `ConstraintChain`. We make use of the method of [Alternating Projections](https://en.wikipedia.org/wiki/Projections_onto_convex_sets), which finds a solution in the intersection of multiple submanifolds by sequentially projecting on every manifold. Normally, this process needs to be repeated multiple times to ensure that all constraints are being met. In practice, it is sufficient to repeat this process only once because the optimizer will require multiple iterations to finally converge, so in the vicinity of the final solution, multiple projections are in fact performed."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
